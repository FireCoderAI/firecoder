name: Build Server

on:
  push:
    branches:
      - "add-ci"
  workflow_dispatch:
    inputs:
      llama-tag:
        description: "llama.cpp tag"
        required: true
        type: string
        default: "b1768"

jobs:
  build-cmake-ubuntu:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: "ggerganov/llama.cpp"
          ref: github.event.inputs.llama-tag
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install build-essential
      - name: Build
        run: |
          mkdir build
          cd build
          cmake .. -DBUILD_SHARED_LIBS=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_NATIVE=OFF
          cmake --build . --config Release --parallel $(nproc)
      - name: Test
        run: |
          cd build
          ctest --verbose --timeout 900
      - name: Upload server
        uses: shallwefootball/upload-s3-action@master
        with:
          aws_key_id: ${{ secrets.AWS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY}}
          aws_bucket: ${{ secrets.AWS_BUCKET }}
          endpoint: ${{ secrets.AWS_ENDPOINT }}
          source_dir: "/build/server"

  build-cmake-ubuntu-cublas:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: "ggerganov/llama.cpp"
          ref: github.event.inputs.llama-tag

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install build-essential

      # - name: Install cuda-toolkit
      #   uses: Jimver/cuda-toolkit@v0.2.11
      #   with:
      #     cuda: "12.2.0"
      #     method: "network"
      #     sub-packages: '["nvcc", "cudart"]'
      #     non-cuda-sub-packages: '["libcublas", "libcublas-dev", "cuda-toolkit"]'
      # - name: Install cuda-toolkit
      #   run: |
      #     wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
      #     sudo dpkg -i cuda-keyring_1.1-1_all.deb
      #     sudo apt-get update
      #     sudo apt-get -y install cuda-toolkit-12-2
      - name: Install cuda-toolkit
        run: |
          sudo apt-get update
          sudo apt-get -y install nvidia-cuda-toolkit
      - name: Build
        id: cmake_build
        run: |
          mkdir build
          cd build
          cmake .. -DBUILD_SHARED_LIBS=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_NATIVE=OFF -DLLAMA_CUBLAS=ON
          cmake --build . --config Release --parallel $(nproc)

  build-cmake-windows:
    runs-on: windows-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: "ggerganov/llama.cpp"
          ref: github.event.inputs.llama-tag
      - name: Windows GitHub Actions environment variables List
        run: env
      - name: Build
        run: |
          mkdir build
          cd build
          cmake .. -DBUILD_SHARED_LIBS=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_NATIVE=OFF
          cmake --build . --config Release --parallel ${env:NUMBER_OF_PROCESSORS}
      - name: Test
        run: |
          cd build
          ctest -C Release --verbose --timeout 900

  build-cmake-windows-cublas:
    runs-on: windows-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: "ggerganov/llama.cpp"
          ref: github.event.inputs.llama-tag

      - name: Install cuda-toolkit
        uses: Jimver/cuda-toolkit@v0.2.11
        with:
          cuda: "12.2.0"
          method: "network"
          sub-packages: '["nvcc", "cudart", "cublas", "cublas_dev", "thrust", "visual_studio_integration"]'

      - name: Build
        id: cmake_build
        run: |
          mkdir build
          cd build
          cmake .. -DBUILD_SHARED_LIBS=ON -DLLAMA_BUILD_SERVER=ON -DLLAMA_NATIVE=OFF -DLLAMA_CUBLAS=ON
          cmake --build . --config Release --parallel ${env:NUMBER_OF_PROCESSORS}

  build-cmake-macOS:
    runs-on: macos-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: "ggerganov/llama.cpp"
          ref: github.event.inputs.llama-tag

      - name: Install dependencies
        run: |
          brew update

      - name: Build
        id: cmake_build
        run: |
          sysctl -a
          mkdir build
          cd build
          cmake -DLLAMA_METAL=OFF ..
          cmake --build . --config Release --parallel $(sysctl -n hw.logicalcpu)

      - name: Test
        id: cmake_test
        run: |
          cd build
          ctest --verbose --timeout 900
  build-cmake-macOS-metal:
    runs-on: macos-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          repository: "ggerganov/llama.cpp"
          ref: github.event.inputs.llama-tag

      - name: Install dependencies
        run: |
          brew update

      - name: Build
        id: cmake_build
        run: |
          sysctl -a
          mkdir build
          cd build
          cmake ..
          cmake --build . --config Release --parallel $(sysctl -n hw.logicalcpu)
